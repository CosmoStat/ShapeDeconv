{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a step by step guide about how to train a deep neural network (DNN) in the DeepDeconv framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the sys.path in order to be able to import our modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../python'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #\"0\" for the 1st GPU or \"1\" to use the 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## extra imports to set GPU options\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "# This line is optional, don't add it unless you really need to set a limit on the memory available for your process\n",
    "# For instance, if you want to train 2 DNNs on the same GPU without one overlapping the memory needed by the other\n",
    "# Change the value to set the percentage of memory allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.75 \n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# Now you can create/load your DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the class of the network. This class must inherit from the DeepNet superclass. The method build_model has to be redefined in the child class with the wanted architecture. In our work, we use the network defined in deconvnNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeepDeconv.deepnetFCS.DeconvNet_custom import DeconvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) The network \n",
    "The purpose of this next code is to create the Dense XCeption U-Net with all the necessary parameters (number of scales,layers, growth rate, weighting factor, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeconvNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a906a9a700c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnetwork_name\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m'_resNet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m dnn = DeconvNet(network_name = network_name, img_rows = 96, img_cols = 96, model_file='', verbose=True,\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mnb_scales\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_scales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_layers_per_block\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_layers_per_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mactivation_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresNet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matrou\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matrou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DeconvNet' is not defined"
     ]
    }
   ],
   "source": [
    "nb_scales = 4 #4\n",
    "growth_rate = 12 #12\n",
    "nb_layers_per_block = [4,5,6,7]#[4,5,6,7]\n",
    "activation_function= 'relu' #'relu'\n",
    "gamma=0  #weighting factor for the loss function\n",
    "shape_constraint=True # or can be shearlet = True or if nothing is specified, both shape_constraint and shearlet are False\n",
    "atrou=False\n",
    "resNet=False\n",
    "layer_string='layer{0}'.format(nb_layers_per_block[0])\n",
    "\n",
    "if shape_constraint:\n",
    "    shape='shape_constraint\n",
    "elif shearlet:\n",
    "    shape='shearlet'\n",
    "else:\n",
    "    shape='noshape'\n",
    "\n",
    "\n",
    "for k in range(1,len(nb_layers_per_block)):\n",
    "    layer_string+='x{0}'.format(nb_layers_per_block[k])\n",
    "network_name='ShapeNet2D_claire_sc{0}_{1}_{2}_growthRate{3}_{4}_gamma{5}'.format(nb_scales,layer_string,\n",
    "                                                                              activation_function,growth_rate,shape,gamma)\n",
    "if resNet:\n",
    "    network_name+='_resNet'\n",
    "\n",
    "dnn = DeconvNet(network_name = network_name, img_rows = 96, img_cols = 96, model_file='', verbose=True,\n",
    "                nb_scales=nb_scales, growth_rate=growth_rate, nb_layers_per_block=nb_layers_per_block, \n",
    "                activation_function=activation_function,resNet=resNet,atrou=atrou,gamma=gamma,\n",
    "                shape_constraint=shape_constraint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-0-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-1-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-10-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-11-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-12-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-13-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-14-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-15-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-16-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-17-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-18-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-19-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-2-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-20-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-3-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-4-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-5-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-6-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-7-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-8-multihdu.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/image-shfl-9-multihdu.fits']\n",
      "['/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_0.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_1.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_10.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_11.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_12.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_13.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_14.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_15.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_16.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_17.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_18.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_19.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_2.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_20.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_3.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_4.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_5.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_6.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_7.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_8.fits', '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/window/Gaussian_window_claire_9.fits']\n"
     ]
    }
   ],
   "source": [
    "#Input the directory containing the fits file\n",
    "data_directory = '/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/'\n",
    "write_path=\"/data/DeepDeconv/data/vsc_euclidpsfs/reshuffle/\"\n",
    "\n",
    "#Retrieves the list of all the files\n",
    "import glob\n",
    "\n",
    "gal_files = glob.glob(data_directory+'image-*-multihdu.fits')\n",
    "gal_files.sort()\n",
    "print(gal_files)\n",
    "\n",
    "#if using windows\n",
    "win_files = glob.glob(write_path+'window/'+'Gaussian*')\n",
    "win_files.sort()\n",
    "print(win_files)\n",
    "\n",
    "\n",
    "SNR = [20,100]#Range of SNR simulated\n",
    "noiseless_img_hdu = 0\n",
    "psf_hdu = 1\n",
    "targets_hdu = 2\n",
    "deconv_mode = 'TIKHONOV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved at /home/cben-ali/Programs/deep-deconv/tutorials/ShapeNet2D_claire_sc4_layer4x5x6x7_relu_growthRate12_preshape_10epochs.hdf5\n",
      "Memory usage for the model + one batch (GB): 2.809000\n",
      "Not using any previous monitored value for checkpoint\n",
      "WARNING:tensorflow:From /local/home/fsureau/miniconda3/envs/tfenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cben-ali/Programs/deep-deconv/python/DeepDeconv/deepnetFCS/DeepNet.py:363: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=10, validation_data=([array([[..., verbose=1, callbacks=[<DeepDeco..., initial_epoch=0, steps_per_epoch=5947)`\n",
      "  initial_epoch=initial_epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1881/5947 [========>.....................] - ETA: 1:23:27 - loss: 1.2312 - weighted_mean_squared_error: 1.2312 - weighted_shape_metric: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "#Train with the image-000-0.fits as validation and all the other files as training set\n",
    "#dnn.train_generator(gal_files[1:], gal_files[0], epochs=20, batch_size=32,\n",
    "#                        nb_img_per_file=10000, validation_set_size=10000,\n",
    "#                        noise_std=None, SNR=SNR, model_file='',\n",
    "#                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu, psf_hdu=psf_hdu,\n",
    "#                        image_dim=96, image_per_row=100,\n",
    "#                        deconv_mode=deconv_mode)\n",
    "#Here the number of epochs is set to 2, should be on the order of 20 at the end\n",
    "\n",
    "dnn.train_generator(gal_files[2:], gal_files[1], epochs=10, batch_size=32,\n",
    "                        model_file='', nb_img_per_file=10000, \n",
    "                        validation_set_size=10000, noise_std=None, SNR=SNR, \n",
    "                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu,\n",
    "                        psf_hdu=psf_hdu, image_dim=96, image_per_row=100,\n",
    "                        deconv_mode=deconv_mode, win_validation_filename=win_files[1],\n",
    "                        win_filename=win_files[2:],win_hdu=0,mom_hdu=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Alexis net : Restart with gamma=0\n",
    "Used to check if training without shape constraint is equivalent to training with shape constraint and gamma=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDeconv.deepnetFCS.DeepNet import DeepNet\n",
    "dnn2=DeepNet(model_file=\"ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_preshape_10epochs.hdf5\",\n",
    "             shape_constraint=True,gamma=0) \n",
    "print(dnn2.model.inputs)\n",
    "print(dnn2.model.input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.train_generator(gal_files[2:], gal_files[1], epochs=10, batch_size=32,\n",
    "                        model_file='ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_test_ref_restart.hdf5', \n",
    "                        nb_img_per_file=10000, \n",
    "                        validation_set_size=10000, noise_std=None, SNR=SNR, \n",
    "                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu,\n",
    "                        psf_hdu=psf_hdu, image_dim=96, image_per_row=100,\n",
    "                        deconv_mode=deconv_mode, win_validation_filename=win_files[1],win_filename=win_files[2:],\n",
    "                        win_hdu=0,mom_hdu=1)#,keep_best_loss=False by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dnn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Alexis net : Restart with gamma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDeconv.deepnetFCS.DeepNet import DeepNet\n",
    "dnn2=DeepNet(model_file=\"ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_preshape_10epochs.hdf5\",\n",
    "             shape_constraint=True,gamma=0.1) \n",
    "print(dnn2.model.inputs)\n",
    "print(dnn2.model.input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.train_generator(gal_files[2:], gal_files[1], epochs=10, batch_size=32,\n",
    "                        model_file='ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_gamma0.1_restart.hdf5', \n",
    "                        nb_img_per_file=10000, \n",
    "                        validation_set_size=10000, noise_std=None, SNR=SNR, \n",
    "                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu,\n",
    "                        psf_hdu=psf_hdu, image_dim=96, image_per_row=100,\n",
    "                        deconv_mode=deconv_mode, win_validation_filename=win_files[1],win_filename=win_files[2:],\n",
    "                        win_hdu=0,mom_hdu=1)#,keep_best_loss=False by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dnn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Alexis net : Restart with gamma=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDeconv.deepnetFCS.DeepNet import DeepNet\n",
    "dnn2=DeepNet(model_file=\"ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_preshape_10epochs.hdf5\",\n",
    "             shape_constraint=True,gamma=0.01) \n",
    "print(dnn2.model.inputs)\n",
    "print(dnn2.model.input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.train_generator(gal_files[2:], gal_files[1], epochs=10, batch_size=32,\n",
    "                        model_file='ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_gamma0.01_restart.hdf5', \n",
    "                        nb_img_per_file=10000, \n",
    "                        validation_set_size=10000, noise_std=None, SNR=SNR, \n",
    "                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu,\n",
    "                        psf_hdu=psf_hdu, image_dim=96, image_per_row=100,\n",
    "                        deconv_mode=deconv_mode, win_validation_filename=win_files[1],win_filename=win_files[2:],\n",
    "                        win_hdu=0,mom_hdu=1)#,keep_best_loss=False by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dnn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Alexis net : Restart with gamma=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDeconv.deepnetFCS.DeepNet import DeepNet\n",
    "dnn2=DeepNet(model_file=\"ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_preshape_10epochs.hdf5\",\n",
    "             shape_constraint=True,gamma=0.001) \n",
    "print(dnn2.model.inputs)\n",
    "print(dnn2.model.input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.train_generator(gal_files[2:], gal_files[1], epochs=10, batch_size=32,\n",
    "                        model_file='ShapeNet2D_FCS_claire_sc4_layer4x5x6x7_relu_growthRate12_gamma0.001_restart.hdf5', \n",
    "                        nb_img_per_file=10000, \n",
    "                        validation_set_size=10000, noise_std=None, SNR=SNR, \n",
    "                        noiseless_img_hdu=noiseless_img_hdu, targets_hdu=targets_hdu,\n",
    "                        psf_hdu=psf_hdu, image_dim=96, image_per_row=100,\n",
    "                        deconv_mode=deconv_mode, win_validation_filename=win_files[1],win_filename=win_files[2:],\n",
    "                        win_hdu=0,mom_hdu=1)#,keep_best_loss=False by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix for GPU options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extra imports to set GPU options\n",
    "#import tensorflow as tf\n",
    "#from keras import backend as k\n",
    "\n",
    "###################################\n",
    "# TensorFlow wizardry\n",
    "#config = tf.ConfigProto()\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "# This line is optional, don't add it unless you really need to set a limit on the memory available for your process\n",
    "# For instance, if you want to train 2 DNNs on the same GPU without one overlapping the memory needed by the other\n",
    "# Change the value to set the percentage of memory allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5 \n",
    "\n",
    "# Create a session with the above options specified.\n",
    "#k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# Now you can create/load your DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with multiple GPUs, Tensorflow will pre-allocate the whole memory of all the GPUs.\n",
    "Use the following to prevent it (only when your station has several GPUs like the SAPPCN63):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #\"0\" for the 1st GPU or \"1\" to use the 2nd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
