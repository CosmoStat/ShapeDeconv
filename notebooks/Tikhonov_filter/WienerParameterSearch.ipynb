{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search for the Wiener Filter Parameter\n",
    "\n",
    "In this notebook, we are going to determine the $\\lambda$ for the Wiener filter using a line search. The value we are looking for is the argmin of:\n",
    "$$\n",
    "\\|Y-H\\ast X_\\lambda\\|^2_2 + \\lambda \\|X_\\lambda\\|^2_2\n",
    "$$\n",
    "where $X_\\lambda$ is the output of the Tikhonov (wiener) Filter $\\mathcal{T}_\\lambda(Y,H)$.\n",
    "\n",
    "> For a sake of simplicity in the line search, we did not not consider a Laplacian regularization but the identity one. In the scope of this work, the precise value of $\\lambda$ is not crucial. Any value of $\\lambda$ that ensures a stable deconvolution is satisfactory. This choice is motivated by the approach in the [ForWaRD](https://ieeexplore.ieee.org/abstract/document/1261329?casa_token=9c8WAbl7hiAAAAAA:dOrXfVmlVGs6RxCdpnGULKOutryQ3n1dRgRc_Yug2Y_oNo4nGzwGKHgivpagDZfMZXigFYS5i2I3) method.\n",
    "\n",
    "## Load requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "# Add library path to PYTHONPATH\n",
    "lib_path = '/gpfswork/rech/xdy/uze68md/GitHub/'\n",
    "path_alphatransform = lib_path+'alpha-transform'\n",
    "path_score = lib_path+'score'\n",
    "sys.path.insert(0, path_alphatransform)\n",
    "sys.path.insert(0, path_score)\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import fft\n",
    "from scipy.ndimage import zoom\n",
    "import cadmos_lib as cl\n",
    "import tensorflow as tf\n",
    "import galsim\n",
    "from galsim import Image\n",
    "import galsim.hsm\n",
    "import galflow as gf\n",
    "from galaxy2galaxy import problems\n",
    "\n",
    "# Functions\n",
    "\n",
    "def recons(batch, interp_factor=2):\n",
    "    \"\"\"Reconstruct observations from images filtered with Tikhonov\"\"\"\n",
    "    # resize Tikhonov images\n",
    "    tikho = np.array([zoom(t[...,0], zoom=interp_factor) for t in batch['inputs_tikho']])\n",
    "    # apply real Fourier transform on them\n",
    "    tikho = np.array([ np.fft.rfft2(t)for t in tikho])\n",
    "    # multiply by the input PSF and divide by the target PSF\n",
    "    recons = np.array([t*c[...,0]/h[...,0] for t,c,h in zip(tikho,batch['psf_cfht'],batch['psf_hst'])])\n",
    "    # apply inverse real Fourier transform on the result\n",
    "    recons = np.array([np.fft.irfft2(r) for r in recons])\n",
    "    return recons,tikho\n",
    "\n",
    "def ir2tf(imp_resp, shape):\n",
    "    \n",
    "\n",
    "    dim = 2\n",
    "    # Zero padding and fill\n",
    "    irpadded = np.zeros(shape)\n",
    "    irpadded[tuple([slice(0, s) for s in imp_resp.shape])] = imp_resp\n",
    "    # Roll for zero convention of the fft to avoid the phase\n",
    "    # problem. Work with odd and even size.\n",
    "    for axis, axis_size in enumerate(imp_resp.shape):\n",
    "\n",
    "        irpadded = np.roll(irpadded,\n",
    "                           shift=-int(np.floor(axis_size / 2)),\n",
    "                           axis=axis)\n",
    "\n",
    "    return fft.rfftn(irpadded, axes=range(-dim, 0))\n",
    "\n",
    "def laplacian(shape):\n",
    "    \n",
    "    impr = np.zeros([3,3])\n",
    "    for dim in range(2):\n",
    "        idx = tuple([slice(1, 2)] * dim +\n",
    "                    [slice(None)] +\n",
    "                    [slice(1, 2)] * (1 - dim))\n",
    "        impr[idx] = np.array([-1.0,\n",
    "                              0.0,\n",
    "                              -1.0]).reshape([-1 if i == dim else 1\n",
    "                                              for i in range(2)])\n",
    "    impr[(slice(1, 2), ) * 2] = 4.0\n",
    "    return ir2tf(impr, shape), impr\n",
    "\n",
    "def laplacian_tf(shape):\n",
    "    return tf.convert_to_tensor(laplacian(shape)[0])\n",
    "\n",
    "def wiener_tf(image, psf, balance, laplacian=True):\n",
    "    r\"\"\"Applies Wiener filter to image.\n",
    "\n",
    "    This function takes an image in the direct space and its corresponding PSF in the\n",
    "    Fourier space and performs a deconvolution using the Wiener Filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image   : 2D TensorFlow tensor\n",
    "        Image in the direct space.\n",
    "    psf     : 2D TensorFlow tensor\n",
    "        PSF in the Fourier space (or K space).\n",
    "    balance : scalar\n",
    "        Weight applied to regularization.\n",
    "    laplacian : boolean\n",
    "        If true the Laplacian regularization is used else the identity regularization \n",
    "        is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The first element is the filtered image in the Fourier space.\n",
    "        The second element is the PSF in the Fourier space (also know as the Transfer\n",
    "        Function).\n",
    "    \"\"\"\n",
    "    trans_func = psf\n",
    "    if laplacian:\n",
    "        reg = laplacian_tf(image.shape)\n",
    "        if psf.shape != reg.shape:\n",
    "            trans_func = tf.signal.rfft2d(tf.signal.ifftshift(tf.cast(psf, 'float32')))\n",
    "        else:\n",
    "            trans_func = psf\n",
    "    \n",
    "    arg1 = tf.cast(tf.math.conj(trans_func), 'complex64')\n",
    "    arg2 = tf.dtypes.cast(tf.math.abs(trans_func),'complex64') ** 2\n",
    "    arg3 = balance\n",
    "    if laplacian:\n",
    "        arg3 *= tf.dtypes.cast(tf.math.abs(laplacian_tf(image.shape)), 'complex64')**2\n",
    "    wiener_filter = arg1 / (arg2 + arg3)\n",
    "    \n",
    "    # Apply wiener in Foutier (or K) space\n",
    "    wiener_applied = wiener_filter * tf.signal.rfft2d(tf.cast(image, 'float32'))\n",
    "    \n",
    "    return wiener_applied, trans_func\n",
    "\n",
    "def pre_proc_unet(dico):\n",
    "    r\"\"\"Preprocess the data and apply the Tikhonov filter on the input galaxy images.\n",
    "\n",
    "    This function takes the dictionnary of galaxy images and PSF for the input and\n",
    "    the target and returns a list containing 2 arrays: an array of galaxy images that\n",
    "    are the output of the Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dico : dictionnary\n",
    "        Array_like means all those objects -- lists, nested lists, etc. --\n",
    "        that can be converted to an array.  We can also refer to\n",
    "        variables like `var1`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list containing 2 arrays: an array of galaxy images that are the output of the\n",
    "        Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    These are written in doctest format, and should illustrate how to\n",
    "    use the function.\n",
    "\n",
    "    >>> from galaxy2galaxy import problems # to list avaible problems run problems.available()\n",
    "    >>> problem128 = problems.problem('attrs2img_cosmos_hst2euclide')\n",
    "    >>> dset = problem128.dataset(Modes.TRAIN, data_dir='attrs2img_cosmos_hst2euclide')\n",
    "    >>> dset = dset.map(pre_proc_unet)\n",
    "    \"\"\"\n",
    "    # First, we add noise\n",
    "    # For the estimation of CFHT noise standard deviation check section 3 of:\n",
    "    # https://github.com/CosmoStat/ShapeDeconv/blob/master/data/CFHT/HST2CFHT.ipynb\n",
    "    sigma_cfht = 23.59\n",
    "    noise = tf.random_normal(shape=tf.shape(dico['inputs']), mean=0.0, stddev=sigma_cfht, dtype=tf.float32)\n",
    "    dico['inputs'] = dico['inputs'] + noise\n",
    "\n",
    "    # Second, we interpolate the image on a finer grid\n",
    "    x_interpolant=tf.image.ResizeMethod.BICUBIC\n",
    "    interp_factor = 2\n",
    "    Nx = 64\n",
    "    Ny = 64\n",
    "    dico['inputs_cfht'] = tf.image.resize(dico['inputs'],\n",
    "                    [Nx*interp_factor,\n",
    "                    Ny*interp_factor],\n",
    "                    method=x_interpolant)\n",
    "    # Since we lower the resolution of the image, we also scale the flux\n",
    "    # accordingly\n",
    "    dico['inputs_cfht'] = dico['inputs_cfht'] / interp_factor**2\n",
    "\n",
    "    balance = 9e-3  # determined using line search\n",
    "    dico['inputs_tikho'], _ = wiener_tf(dico['inputs_cfht'][...,0], dico['psf_cfht'][...,0], balance)\n",
    "    dico['inputs_tikho'] = tf.expand_dims(dico['inputs_tikho'], axis=0)\n",
    "    psf_hst = tf.reshape(dico['psf_hst'], [dico['psf_hst'].shape[-1],*dico['psf_hst'].shape[:2]])\n",
    "    psf_hst = tf.cast(psf_hst, 'complex64')\n",
    "    # gf.kconvolve performs a convolution in the K (Fourier) space\n",
    "    # inputs are given in K space\n",
    "    # the output is in the direct space\n",
    "    dico['inputs_tikho'] = gf.kconvolve(dico['inputs_tikho'], psf_hst,zero_padding_factor=1,interp_factor=interp_factor)\n",
    "    dico['inputs_tikho'] = dico['inputs_tikho'][0,...]\n",
    "\n",
    "    return dico\n",
    "\n",
    "def make_preproc(balance):\n",
    "    def pre_proc_unet(dico):\n",
    "        r\"\"\"Preprocess the data and apply the Tikhonov filter on the input galaxy images.\n",
    "\n",
    "        This function takes the dictionnary of galaxy images and PSF for the input and\n",
    "        the target and returns a list containing 2 arrays: an array of galaxy images that\n",
    "        are the output of the Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dico : dictionnary\n",
    "            Array_like means all those objects -- lists, nested lists, etc. --\n",
    "            that can be converted to an array.  We can also refer to\n",
    "            variables like `var1`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list containing 2 arrays: an array of galaxy images that are the output of the\n",
    "            Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        These are written in doctest format, and should illustrate how to\n",
    "        use the function.\n",
    "\n",
    "        >>> from galaxy2galaxy import problems # to list avaible problems run problems.available()\n",
    "        >>> problem128 = problems.problem('attrs2img_cosmos_hst2euclide')\n",
    "        >>> dset = problem128.dataset(Modes.TRAIN, data_dir='attrs2img_cosmos_hst2euclide')\n",
    "        >>> dset = dset.map(pre_proc_unet)\n",
    "        \"\"\"\n",
    "        # First, we add noise\n",
    "        # For the estimation of CFHT noise standard deviation check section 3 of:\n",
    "        # https://github.com/CosmoStat/ShapeDeconv/blob/master/data/CFHT/HST2CFHT.ipynb\n",
    "        sigma_cfht = 23.59\n",
    "        noise = tf.random_normal(shape=tf.shape(dico['inputs']), mean=0.0, stddev=sigma_cfht, dtype=tf.float32)\n",
    "        dico['inputs'] = dico['inputs'] + noise\n",
    "    \n",
    "        # Second, we interpolate the image on a finer grid\n",
    "        x_interpolant=tf.image.ResizeMethod.BICUBIC\n",
    "        interp_factor = 2\n",
    "        Nx = 64\n",
    "        Ny = 64\n",
    "        dico['inputs_cfht'] = tf.image.resize(dico['inputs'],\n",
    "                        [Nx*interp_factor,\n",
    "                        Ny*interp_factor],\n",
    "                        method=x_interpolant)\n",
    "        # Since we lower the resolution of the image, we also scale the flux\n",
    "        # accordingly\n",
    "        dico['inputs_cfht'] = dico['inputs_cfht'] / interp_factor**2\n",
    "\n",
    "        # balance = 10**(-2.16)  # best after old grid search performed by Hippolyte\n",
    "        dico['inputs_tikho'], _ = wiener_tf(dico['inputs_cfht'][...,0], dico['psf_cfht'][...,0], balance)\n",
    "        dico['inputs_tikho'] = tf.expand_dims(dico['inputs_tikho'], axis=0)\n",
    "        psf_hst = tf.reshape(dico['psf_hst'], [dico['psf_hst'].shape[-1],*dico['psf_hst'].shape[:2]])\n",
    "        psf_hst = tf.cast(psf_hst, 'complex64')\n",
    "        # gf.kconvolve performs a convolution in the K (Fourier) space\n",
    "        # inputs are given in K space\n",
    "        # the output is in the direct space\n",
    "        dico['inputs_tikho'] = gf.kconvolve(dico['inputs_tikho'], psf_hst,zero_padding_factor=1,interp_factor=interp_factor)\n",
    "        dico['inputs_tikho'] = dico['inputs_tikho'][0,...]\n",
    "\n",
    "        return dico\n",
    "    return pre_proc_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an instance of the hsc_problem\n",
    "Modes = tf.estimator.ModeKeys\n",
    "problem128 = problems.problem('attrs2img_cosmos_cfht2hst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Precheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /linkhome/rech/gencea01/uze68md/.local/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py:651: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-c507f720ec40>:7: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "dset = problem128.dataset(Modes.EVAL, data_dir='/gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/')\n",
    "dset = dset.repeat()\n",
    "dset = dset.map(make_preproc(10**(-2)))\n",
    "n_batch = 128\n",
    "dset = dset.batch(n_batch)\n",
    "# Build an iterator over the dataset\n",
    "iterator = dset.make_one_shot_iterator().get_next()\n",
    "sess = tf.Session()\n",
    "# Initialize batch\n",
    "batch = sess.run(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude Line Search\n",
    "\n",
    "We start by performing a grid search to determine the magnitude (exponent) of the $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "The optimal magnitude is -2\n"
     ]
    }
   ],
   "source": [
    "mags = [-3,-2,-1,0,1,2,3] # magnitudes\n",
    "mag_opt = mags[0]\n",
    "loss_min = -1\n",
    "all_losses = []\n",
    "\n",
    "interp_factor = 2.0\n",
    "\n",
    "# resize observations\n",
    "obs = np.array([zoom(i[...,0], zoom=interp_factor) for i in batch['inputs']])\n",
    "\n",
    "# make Laplacian operator\n",
    "lap_filter,_ = laplacian(obs.shape[-2:])\n",
    "\n",
    "for mag in mags:\n",
    "    \n",
    "    dset = problem128.dataset(Modes.EVAL, data_dir='/gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/')\n",
    "    dset = dset.repeat()\n",
    "    dset_tmp = dset.map(make_preproc(10**mag))\n",
    "\n",
    "    n_batch = 128\n",
    "    \n",
    "    dset_tmp = dset_tmp.batch(n_batch)\n",
    "    # Build an iterator over the dataset\n",
    "    iterator = dset_tmp.make_one_shot_iterator().get_next()\n",
    "    sess = tf.Session()\n",
    "    # Initialize batch\n",
    "    batch = sess.run(iterator)\n",
    "    tikh_recons, tikho = recons(batch, interp_factor=interp_factor)\n",
    "    error = obs - tikh_recons\n",
    "    mse_list = np.array([np.mean(e**2) for e in error])\n",
    "    loss_list = np.array([m + 10**mag * np.mean(np.fft.irfft2(t)**2) for m,t in zip(mse_list, tikho)])\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    # concatenate losses\n",
    "    all_losses += [loss]\n",
    "    \n",
    "mag = mags[np.argmin(all_losses)]\n",
    "print(\"The optimal magnitude is {}\".format(mag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significand Line Search\n",
    "\n",
    "Now we perform a line search to determine the significand (the first digit) of $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 2\n",
      "The optimal value is 0.09\n"
     ]
    }
   ],
   "source": [
    "values1 = np.array([5, 6 ,7 ,8 , 9])\n",
    "values2 = np.array([1,2,3,4,5,6,7,8,9])\n",
    "values = np.hstack([values1 * 10**(mag-1), values2 * 10**mag])\n",
    "\n",
    "loss_min = -1\n",
    "all_losses = []\n",
    "\n",
    "interp_factor = 2.0\n",
    "\n",
    "for v in values:\n",
    "    \n",
    "    dset = problem128.dataset(Modes.EVAL, data_dir='/gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/')\n",
    "    dset = dset.repeat()\n",
    "    dset_tmp = dset.map(make_preproc(v))\n",
    "\n",
    "    n_batch = 128\n",
    "    \n",
    "    dset_tmp = dset_tmp.batch(n_batch)\n",
    "    # Build an iterator over the dataset\n",
    "    iterator = dset_tmp.make_one_shot_iterator().get_next()\n",
    "    sess = tf.Session()\n",
    "    # Initialize batch\n",
    "    batch = sess.run(iterator)\n",
    "    tikh_recons,tikho = recons(batch, interp_factor=interp_factor)\n",
    "    error = obs - tikh_recons\n",
    "    mse_list = np.array([np.mean(e**2) for e in error])\n",
    "    loss_list = np.array([m + 10**mag * np.mean(np.fft.irfft2(t)**2) for m,t in zip(mse_list, tikho)])\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    # concatenate losses\n",
    "    all_losses += [loss]\n",
    "    \n",
    "value = values[np.argmin(all_losses)]\n",
    "print(\"The optimal value is {}\".format(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
