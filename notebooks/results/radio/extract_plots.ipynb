{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Tikhonet Trained\n",
    "\n",
    "In this Notebook we are going to evaluate the performance of a [Tikhonet](https://arxiv.org/pdf/1911.00443.pdf) trained.\n",
    "\n",
    "## Required Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "# Add library path to PYTHONPATH\n",
    "lib_path = '/gpfswork/rech/xdy/uze68md/GitHub/'\n",
    "path_alphatransform = lib_path+'alpha-transform'\n",
    "path_score = lib_path+'score'\n",
    "sys.path.insert(0, path_alphatransform)\n",
    "sys.path.insert(0, path_score)\n",
    "data_path = '/gpfswork/rech/xdy/uze68md/data/'\n",
    "model_dir = '/gpfswork/rech/xdy/uze68md/trained_models/model_meerkat_64/'\n",
    "\n",
    "# Function\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import fft\n",
    "import cadmos_lib as cl\n",
    "import tensorflow as tf\n",
    "import galsim\n",
    "from galsim import Image\n",
    "import galsim.hsm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_path+\"meerkat_batch.pkl\", \"rb\")\n",
    "batch = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Batches and Concatenate Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()\n",
    "n_batch, Nx, Ny = batch['targets'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Apply Trained Model on Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['restored_residual', 'restored', 'restored_isotropic', 'residual', 'skymodel', 'skymodel_list', 'sigma_flags'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = open(data_path+\"clean_results.pkl\", \"rb\")\n",
    "clean = pickle.load(g)\n",
    "g.close()\n",
    "\n",
    "clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# load Tikhonet results\n",
    "model_name_g0 = 'tikhonet_None-constraint_scales-4_steps-3125_epochs-10_growth_rate-12_batch_size-32_activationfunction-relu'\n",
    "model_name_g05 = 'tikhonet_multi-constraint_scales-4_gamma-0.5_shearlet-3_steps-3125_epochs-10_growth_rate-12_batch_size-32_activationfunction-relu'\n",
    "\n",
    "model_g0 = tf.keras.models.load_model(model_dir+model_name_g0, compile=False)\n",
    "model_g05 = tf.keras.models.load_model(model_dir+model_name_g05, compile=False)\n",
    "\n",
    "inputs_model = np.expand_dims(np.array([crop_center(im,64,64) for im in batch['inputs_tikho']]),axis=-1)\n",
    "\n",
    "res_g0 = model_g0(inputs_model)\n",
    "res_g05 = model_g05(inputs_model)\n",
    "\n",
    "res_g0 = np.array([np.pad(im,32,constant_values=0) for im in tf.keras.backend.eval(res_g0)[...,0]])#np.pad(tf.keras.backend.eval(res_g0)[...,0], 32, constant_values=0)\n",
    "res_g05 = np.array([np.pad(im,32,constant_values=0) for im in tf.keras.backend.eval(res_g05)[...,0]])#np.pad(tf.keras.backend.eval(res_g05)[...,0], 32, constant_values=0)\n",
    "\n",
    "# load CLEAN\n",
    "res_cl = clean['restored']\n",
    "res_cl_iso = clean['restored_isotropic']\n",
    "\n",
    "# load SCORE\n",
    "res_s0 = np.load(data_path+'score_radio_tikho_g0_1.npy')\n",
    "res_s2 = np.load(data_path+'score_radio_tikho_g05_1.npy')\n",
    "\n",
    "for i in range(3):\n",
    "    res_s0 = np.concatenate((res_s0,np.load(data_path+'score_radio_tikho_g0_{}.npy'.format(i+2))), axis=0)\n",
    "    res_s2 = np.concatenate((res_s2,np.load(data_path+'score_radio_tikho_g05_{}.npy'.format(i+2))), axis=0)\n",
    "res_s0 = np.array(res_s0)\n",
    "res_s2 = np.array(res_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop images to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['inputs_tikho'] = np.array([crop_center(im,64,64) for im in batch['inputs_tikho']])\n",
    "batch['inputs'] = np.array([crop_center(im,64,64) for im in batch['inputs']])\n",
    "batch['targets'] = np.array([crop_center(im,64,64) for im in batch['targets']])\n",
    "res_g0 = np.array([crop_center(im,64,64) for im in res_g0])\n",
    "res_g05 = np.array([crop_center(im,64,64) for im in res_g05])\n",
    "res_cl = np.array([crop_center(im,64,64) for im in res_cl])\n",
    "res_cl_iso = np.array([crop_center(im,64,64) for im in res_cl_iso])\n",
    "res_s0 = np.array([crop_center(im,64,64) for im in res_s0])\n",
    "res_s2 = np.array([crop_center(im,64,64) for im in res_s2])\n",
    "\n",
    "# flag galaxies where clean did not detect signal\n",
    "batch['inputs_tikho'] = batch['inputs_tikho'][clean['sigma_flags']]\n",
    "batch['inputs'] = batch['inputs'][clean['sigma_flags']]\n",
    "batch['targets'] = batch['targets'][clean['sigma_flags']]\n",
    "res_g0 = res_g0[clean['sigma_flags']]\n",
    "res_g05 = res_g05[clean['sigma_flags']]\n",
    "res_cl = res_cl[clean['sigma_flags']]\n",
    "res_cl_iso = res_cl_iso[clean['sigma_flags']]\n",
    "res_s0 = res_s0[clean['sigma_flags']]\n",
    "res_s2 = res_s2[clean['sigma_flags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = cl.makeUi(*batch['inputs_tikho'].shape[1:3])\n",
    "im_size = 64\n",
    "scale = 1.5\n",
    "\n",
    "def relative_mse(solution, ground_truth):\n",
    "    relative_mse = ((solution-ground_truth)**2).mean()/ \\\n",
    "                         (ground_truth**2).mean()\n",
    "    return relative_mse\n",
    "def MSE(X1,X2,norm=False):\n",
    "    #Computes the relative MSE\n",
    "    temp = 1\n",
    "    if norm:\n",
    "        temp = np.mean(X2**2)\n",
    "    return np.mean((X1-X2)**2)/temp\n",
    "\n",
    "def MSE_obj(obj1,obj2,norm=False):\n",
    "    return np.array([MSE(o1,o2,norm) for o1,o2 in zip(obj1,obj2)])\n",
    "\n",
    "def EllipticalGaussian(e1, e2, sig, xc=im_size//2, yc=im_size//2, stamp_size=(im_size,im_size)):\n",
    "    # compute centered grid\n",
    "    ranges = np.array([np.arange(i) for i in stamp_size])\n",
    "    x = np.outer(ranges[0] - xc, np.ones(stamp_size[1]))\n",
    "    y = np.outer(np.ones(stamp_size[0]),ranges[1] - yc)\n",
    "    # shift it to match centroid\n",
    "    xx = (1-e1/2)*x - e2/2*y\n",
    "    yy = (1+e1/2)*y - e2/2*x\n",
    "    # compute elliptical gaussian\n",
    "    return np.exp(-(xx ** 2 + yy ** 2) / (2 * sig ** 2))\n",
    "\n",
    "def get_moments(images, bool_window=False):\n",
    "    g_list,error_flag_list=[],[]\n",
    "    if bool_window:\n",
    "        window_list = []\n",
    "        window_flag_list = []\n",
    "    for image in images:\n",
    "        error_flag = True\n",
    "        #create a galsim version of the data\n",
    "        image_galsim = galsim.Image(image,scale=scale)\n",
    "        #estimate the moments of the observation image\n",
    "        shape = galsim.hsm.FindAdaptiveMom(image_galsim\n",
    "                                         ,guess_centroid=galsim.PositionD(im_size//2,im_size//2)\n",
    "                                         ,strict=False)\n",
    "        if bool_window:\n",
    "            k_sigma = 1.2 #scale up the size of the Gaussian window to make it able to capture more useful signal\n",
    "            window = EllipticalGaussian(-1.*shape.observed_shape.e1, shape.observed_shape.e2 #convention fix:\n",
    "                                                                                             #e1 sign swap\n",
    "                                 ,shape.moments_sigma*k_sigma # convention fix: swap x and y and origin at (0,0)\n",
    "                                 ,shape.moments_centroid.y-1, shape.moments_centroid.x-1\n",
    "                                 ,image.shape)\n",
    "            window_flag = bool(shape.moments_status+1)\n",
    "        g = np.array([shape.observed_shape.g1, shape.observed_shape.g2])\n",
    "        if shape.error_message:# or np.linalg.norm(shape.corrected_g1+shape.corrected_cl_iso*1j)>1:\n",
    "            error_flag = False\n",
    "        error_flag_list += [error_flag]\n",
    "        g_list += [g]\n",
    "        if bool_window:\n",
    "            window_list += [window]\n",
    "            window_flag_list += [window_flag]\n",
    "    output = [np.array(g_list).T,np.array(error_flag_list)]\n",
    "    if bool_window:\n",
    "        output += [np.array([window_list])[0],np.array([window_flag_list])[0]]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pixel errors\n",
    "mse_g0 = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_g0)])\n",
    "mse_g05 = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_g05)])\n",
    "mse_cl_iso = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_cl_iso)])\n",
    "mse_cl = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_cl)])\n",
    "mse_s0 = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_s0)])\n",
    "mse_s2 = np.array([MSE_obj(est,true) for true,est in zip(batch['targets'], res_s2)])\n",
    "\n",
    "# compute relative pixel errors\n",
    "mse_g0_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_g0)])\n",
    "mse_g05_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_g05)])\n",
    "mse_cl_iso_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_cl_iso)])\n",
    "mse_cl_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_cl)])\n",
    "mse_s0_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_s0)])\n",
    "mse_s2_rel = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_s2)])\n",
    "\n",
    "# estimate moments\n",
    "mom_g0,_ = get_moments(res_g0)\n",
    "mom_g05,_ = get_moments(res_g05)\n",
    "mom_cl_iso,_ = get_moments(res_cl_iso)\n",
    "mom_cl,_ = get_moments(res_cl)\n",
    "mom_s0,_ = get_moments(res_s0)\n",
    "mom_s2,_ = get_moments(res_s2)\n",
    "mom_true,_,windows, window_flags = get_moments(batch['targets'],bool_window=True)\n",
    "\n",
    "# estimate flux\n",
    "flux_g0 = np.array([gal.sum() for gal in res_g0]).T\n",
    "flux_g05 = np.array([gal.sum() for gal in res_g05]).T\n",
    "flux_cl_iso = np.array([gal.sum() for gal in res_cl_iso]).T\n",
    "flux_cl = np.array([gal.sum() for gal in res_cl]).T\n",
    "flux_s0 = np.array([gal.sum() for gal in res_s0]).T\n",
    "flux_s2 = np.array([gal.sum() for gal in res_s2]).T\n",
    "flux_true = np.array([gal.sum()  for gal in batch['targets']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute adapative moments errors\n",
    "mom_err_g0 = mom_g0-mom_true\n",
    "mom_err_g05 = mom_g05-mom_true\n",
    "mom_err_cl_iso = mom_cl_iso-mom_true\n",
    "mom_err_cl = mom_cl-mom_true\n",
    "mom_err_s0 = mom_s0-mom_true\n",
    "mom_err_s2 = mom_s2-mom_true\n",
    "\n",
    "#compute flux relative errors\n",
    "flux_err_g0 = np.abs(flux_g0 - flux_true) / flux_true\n",
    "flux_err_g05 = np.abs(flux_g05 - flux_true) /flux_true\n",
    "flux_err_cl = np.abs(flux_cl - flux_true) / flux_true\n",
    "flux_err_cl_iso = np.abs(flux_cl_iso - flux_true) /flux_true\n",
    "flux_err_s0 = np.abs(flux_s0 - flux_true) / flux_true\n",
    "flux_err_s2 = np.abs(flux_s2 - flux_true) /flux_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = [flux_s0, flux_s2, flux_g0, flux_g05, flux_cl, flux_cl_iso]\n",
    "mse_abs = [mse_s0, mse_s2, mse_g0, mse_g05, mse_cl, mse_cl_iso]\n",
    "mse_rel = [mse_s0_rel, mse_s2_rel, mse_g0_rel, mse_g05_rel, mse_cl_rel, mse_cl_iso_rel]\n",
    "mom = [mom_s0, mom_s2, mom_g0, mom_g05, mom_cl, mom_cl_iso]\n",
    "measures = [flux, mse_abs, mse_rel, mom]\n",
    "measure_names = ['flux', 'mse_abs', 'mse_rel', 'mom']\n",
    "methods = ['sparsity', 'score', 'tikhonet', 'tikhonet_sc', 'clean', 'clean_iso']\n",
    "snr = np.array([np.max(gal) / cl.sigma_mad(gal) for gal in batch['inputs']])\n",
    "\n",
    "data = {}\n",
    "\n",
    "# fill dictionnary\n",
    "for i, measure in enumerate(measures):\n",
    "    data[measure_names[i]] = {}\n",
    "    for j, method in enumerate(methods):\n",
    "        data[measure_names[i]][method] = measure[j] \n",
    "\n",
    "# add remaining keys\n",
    "data['windows'] = windows\n",
    "data['window_flags'] = window_flags\n",
    "data['sigma_flags'] = clean['sigma_flags']\n",
    "data['flux']['true'] = flux_true\n",
    "data['mom']['true'] = mom_true\n",
    "data['snr'] = snr\n",
    "\n",
    "# save dictionnary\n",
    "f = open(data_path+\"meerkat3600_data.pkl\",\"wb\")\n",
    "pickle.dump(data,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
