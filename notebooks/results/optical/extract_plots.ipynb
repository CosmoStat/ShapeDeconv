{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Tikhonet Trained\n",
    "\n",
    "In this Notebook we are going to evaluate the performance of a [Tikhonet](https://arxiv.org/pdf/1911.00443.pdf) trained.\n",
    "\n",
    "## Required Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "# Add library path to PYTHONPATH\n",
    "lib_path = '/gpfswork/rech/xdy/uze68md/GitHub/'\n",
    "path_alphatransform = lib_path+'alpha-transform'\n",
    "path_score = lib_path+'score'\n",
    "sys.path.insert(0, path_alphatransform)\n",
    "sys.path.insert(0, path_score)\n",
    "data_path = '/gpfswork/rech/xdy/uze68md/data/'\n",
    "model_dir = '/gpfswork/rech/xdy/uze68md/trained_models/model_cfht/'\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import fft\n",
    "import cadmos_lib as cl\n",
    "import tensorflow as tf\n",
    "import galsim\n",
    "from galsim import Image\n",
    "import galsim.hsm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Comparison Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_path+\"cfht_batch.pkl\", \"rb\")\n",
    "batch = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#correct tikhonov inputs normalisation factor\n",
    "norm_factor = 4e3\n",
    "batch['inputs_tikho'] *= norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Apply Trained Model on Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tikhonet_None-constraint_scales-4_steps-625_epochs-10_growth_rate-12_batch_size-128_activationfunction-relu'\n",
    "# g: gamma (trade-off parameter of the shape constraint)\n",
    "model_g05_name = 'tikhonet_multi-constraint_scales-4_gamma-0.5_shearlet-3_steps-625_epochs-10_growth_rate-12_batch_size-128_activationfunction-relu'\n",
    "model = tf.keras.models.load_model(model_dir+model_name, compile=False)\n",
    "model_g05 = tf.keras.models.load_model(model_dir+model_g05_name, compile=False)\n",
    "res = model(np.expand_dims(batch['inputs_tikho'], axis=-1))\n",
    "res_np = tf.keras.backend.eval(res)[...,0]\n",
    "res_g05 = model_g05(np.expand_dims(batch['inputs_tikho'], axis=-1))\n",
    "res_g05_np = tf.keras.backend.eval(res_g05)[...,0]\n",
    "score_g0 = np.load(data_path+'score_g0.npy')\n",
    "score_g1 = np.load(data_path+'score_g1.npy')\n",
    "\n",
    "# generate the psfs in the spatial domain\n",
    "psf_hst = np.fft.ifftshift(np.fft.irfft2(batch['psf_hst'][0]))\n",
    "psf_tile_cfht = np.array([np.fft.ifftshift(np.fft.irfft2(p)) for p in batch['psf_cfht']])\n",
    "# make psf tiles\n",
    "psf_tile_hst = np.repeat(psf_hst[np.newaxis, :, :], batch['psf_hst'].shape[0], axis=0)\n",
    "# psf_tile_cfht = np.repeat(psf_cfht[np.newaxis, :, :], k_batch*n_batch, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Analyzing Results\n",
    "\n",
    "### Define Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "scale = 0.1\n",
    "\n",
    "def EllipticalGaussian(e1, e2, sig, xc=im_size//2, yc=im_size//2, stamp_size=(im_size,im_size)):\n",
    "    # compute centered grid\n",
    "    ranges = np.array([np.arange(i) for i in stamp_size])\n",
    "    x = np.outer(ranges[0] - xc, np.ones(stamp_size[1]))\n",
    "    y = np.outer(np.ones(stamp_size[0]),ranges[1] - yc)\n",
    "    # shift it to match centroid\n",
    "    xx = (1-e1/2)*x - e2/2*y\n",
    "    yy = (1+e1/2)*y - e2/2*x\n",
    "    # compute elliptical gaussian\n",
    "    return np.exp(-(xx ** 2 + yy ** 2) / (2 * sig ** 2))\n",
    "\n",
    "def relative_mse(solution, ground_truth):\n",
    "    relative_mse = ((solution-ground_truth)**2).mean()/ \\\n",
    "                         (ground_truth**2).mean()\n",
    "    return relative_mse\n",
    "\n",
    "\n",
    "\n",
    "def get_KSB_ell(image,psf):\n",
    "    error_flag = True\n",
    "    #create a galsim version of the data\n",
    "    image_galsim = Image(image,scale=scale)\n",
    "    psf_galsim = Image(psf,scale=scale)\n",
    "    #estimate the moments of the observation image\n",
    "    ell=galsim.hsm.EstimateShear(image_galsim\n",
    "                                 ,psf_galsim,shear_est='KSB'\n",
    "                                 ,guess_centroid=galsim.PositionD(im_size//2,im_size//2)\n",
    "                                 ,strict=False)\n",
    "    if ell.error_message != '':\n",
    "        error_flag = False\n",
    "    return ell#,error_flag\n",
    "\n",
    "def get_KSB_g(images,psfs):\n",
    "    g_list,error_flag_list=[],[]\n",
    "    for image,psf in zip(images,psfs):\n",
    "        error_flag = True\n",
    "        #create a galsim version of the data\n",
    "        image_galsim = galsim.Image(image,scale=scale)\n",
    "        # CHECK ADAPTIVE MOMENTS\n",
    "        psf_galsim = galsim.Image(psf,scale=scale)\n",
    "        #estimate the moments of the observation image\n",
    "        shape = galsim.hsm.EstimateShear(image_galsim\n",
    "                                         ,psf_galsim,shear_est='KSB'\n",
    "                                         ,guess_centroid=galsim.PositionD(im_size//2,im_size//2)\n",
    "                                         ,strict=False)\n",
    "        g = np.array([shape.corrected_g1, shape.corrected_g2])\n",
    "#        g = np.array([shape.observed_shape.g1, shape.observed_shape.g2])\n",
    "        if shape.error_message:# or np.linalg.norm(shape.corrected_g1+shape.corrected_g2*1j)>1:\n",
    "            error_flag = False\n",
    "        error_flag_list += [error_flag]\n",
    "        g_list += [g]\n",
    "    return np.array(g_list).T,np.array(error_flag_list)\n",
    "\n",
    "def get_moments(images, bool_window=False):\n",
    "    g_list,error_flag_list=[],[]\n",
    "    if bool_window:\n",
    "        window_list = []\n",
    "        window_flag_list = []\n",
    "    for image in images:\n",
    "        error_flag = True\n",
    "        #create a galsim version of the data\n",
    "        image_galsim = galsim.Image(image,scale=scale)\n",
    "        #estimate the moments of the observation image\n",
    "        shape = galsim.hsm.FindAdaptiveMom(image_galsim\n",
    "                                         ,guess_centroid=galsim.PositionD(im_size//2,im_size//2)\n",
    "                                         ,strict=False)\n",
    "        if bool_window:\n",
    "            k_sigma = 1.2 #scale up the size of the Gaussian window to make it able to capture more useful signal\n",
    "            window = EllipticalGaussian(-1.*shape.observed_shape.e1, shape.observed_shape.e2 #convention fix:\n",
    "                                                                                             #e1 sign swap\n",
    "                                 ,shape.moments_sigma*k_sigma # convention fix: swap x and y and origin at (0,0)\n",
    "                                 ,shape.moments_centroid.y-1, shape.moments_centroid.x-1\n",
    "                                 ,image.shape)\n",
    "            window_flag = bool(shape.moments_status+1)\n",
    "        g = np.array([shape.observed_shape.g1, shape.observed_shape.g2])\n",
    "        if shape.error_message:# or np.linalg.norm(shape.corrected_g1+shape.corrected_g2*1j)>1:\n",
    "            error_flag = False\n",
    "        error_flag_list += [error_flag]\n",
    "        g_list += [g]\n",
    "        if bool_window:\n",
    "            window_list += [window]\n",
    "            window_flag_list += [window_flag]\n",
    "    output = [np.array(g_list).T,np.array(error_flag_list)]\n",
    "    if bool_window:\n",
    "        output += [np.array([window_list])[0],np.array([window_flag_list])[0]]\n",
    "    return output\n",
    "\n",
    "def g_to_e(g1,g2):\n",
    "    shear = galsim.Shear(g1=g1,g2=g2)\n",
    "    ell = -shear.e1, shear.e2 #reverse the signe of e_1 to get our conventions\n",
    "    return ell\n",
    "\n",
    "def MSE(X1,X2,norm=False):\n",
    "    #Computes the relative MSE\n",
    "    temp = 1\n",
    "    if norm:\n",
    "        temp = np.mean(X2**2)\n",
    "    return np.mean((X1-X2)**2)/temp\n",
    "\n",
    "def MSE_obj(obj1,obj2,norm=False):\n",
    "    return np.array([MSE(o1,o2,norm) for o1,o2 in zip(obj1,obj2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Adaptive Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate adaptive moments\n",
    "mom_g0,_ = get_moments(res_np)\n",
    "mom_s0,_ = get_moments(score_g0)\n",
    "mom_s1,_ = get_moments(score_g1)\n",
    "mom_g05,_ = get_moments(res_g05_np)\n",
    "mom_hst,_,windows, window_flags = get_moments(batch['targets'],bool_window=True)\n",
    "\n",
    "# estimate flux\n",
    "flux_g0 = np.array([gal.sum() for gal in res_np]).T\n",
    "flux_g05 = np.array([gal.sum() for gal in res_g05_np]).T\n",
    "flux_s0 = np.array([gal.sum() for gal in score_g0]).T\n",
    "flux_s1 = np.array([gal.sum() for gal in score_g1]).T\n",
    "flux_true = np.array([gal.sum()  for gal in batch['targets']]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Moments and Absolute Pixel Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relative pixel errors\n",
    "mse_g0 = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_np)])\n",
    "mse_s0 = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], score_g0)])\n",
    "mse_s1 = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], score_g1)])\n",
    "mse_g05 = np.array([relative_mse(est,true) for true,est in zip(batch['targets'], res_g05_np)])\n",
    "\n",
    "# compute winodwed pixel relative errors\n",
    "mse_g0_w = np.array([relative_mse(est*w,true*w) for true,est,w in zip(batch['targets'], res_np,windows)])\n",
    "mse_s0_w = np.array([relative_mse(est*w,true*w) for true,est,w in zip(batch['targets'], score_g0,windows)])\n",
    "mse_s1_w = np.array([relative_mse(est*w,true*w) for true,est,w in zip(batch['targets'], score_g1,windows)])\n",
    "mse_g05_w = np.array([relative_mse(est*w,true*w) for true,est,w in zip(batch['targets'], res_g05_np,windows)])\n",
    "\n",
    "# compute adapative moments errors\n",
    "mom_err_g0 = mom_g0-mom_hst\n",
    "mom_err_s0 = mom_s0-mom_hst\n",
    "mom_err_s1 = mom_s1-mom_hst\n",
    "mom_err_g05 = mom_g05-mom_hst\n",
    "\n",
    "#compute flux relative errors\n",
    "flux_err_g0 = np.abs(flux_g0 - flux_true) / flux_true\n",
    "flux_err_g05 = np.abs(flux_g05 - flux_true) /flux_true\n",
    "flux_err_s0 = np.abs(flux_s0 - flux_true) / flux_true\n",
    "flux_err_s1 = np.abs(flux_s1 - flux_true) /flux_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = [flux_s0, flux_s1, flux_g0, flux_g05]\n",
    "mse = [mse_s0, mse_s1, mse_g0, mse_g05]\n",
    "mse_w = [mse_s0_w, mse_s1_w, mse_g0_w, mse_g05_w]\n",
    "mom = [mom_s0, mom_s1, mom_g0, mom_g05]\n",
    "measures = [flux, mse, mse_w, mom]\n",
    "measure_names = ['flux', 'mse', 'mse_w', 'mom']\n",
    "methods = ['sparsity', 'score', 'tikhonet', 'tikhonet_sc']\n",
    "\n",
    "data = {}\n",
    "\n",
    "# fill dictionnary\n",
    "for i, measure in enumerate(measures):\n",
    "    data[measure_names[i]] = {}\n",
    "    for j, method in enumerate(methods):\n",
    "        data[measure_names[i]][method] = measure[j] \n",
    "\n",
    "# add remaining keys\n",
    "data['windows'] = windows\n",
    "data['window_flags'] = window_flags\n",
    "data['flux']['true'] = flux_true\n",
    "data['mom']['true'] = mom_hst\n",
    "data['mag_auto'] = batch['mag_auto']\n",
    "\n",
    "# save dictionnary\n",
    "f = open(data_path+\"cfht_data.pkl\",\"wb\")\n",
    "pickle.dump(data,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Errors per Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_s0 = r'Sparsity'\n",
    "label_s1 = r'SCORE'\n",
    "label_g0 = r'Tikhonet'\n",
    "label_g05 = r'Tikhonet + MW'\n",
    "\n",
    "color_g0 = 'green'\n",
    "color_g05 = 'darkgreen'\n",
    "color_s0 = 'blue'\n",
    "color_s1 = 'darkblue'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
