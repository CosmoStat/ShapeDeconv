{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Test Batch\n",
    "\n",
    "In this notebook we show how we generate a batch to be able to compare different reconstruction methods with the exact same data.\n",
    "\n",
    "## Create Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: IERSStaleWarning: leap-second file is expired. [astropy.utils.iers.iers]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "# Add library path to PYTHONPATH\n",
    "lib_path = '/gpfswork/rech/xdy/uze68md/GitHub/'\n",
    "path_alphatransform = lib_path+'alpha-transform'\n",
    "path_score = lib_path+'score'\n",
    "sys.path.insert(0, path_alphatransform)\n",
    "sys.path.insert(0, path_score)\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import fft\n",
    "import cadmos_lib as cl\n",
    "import tensorflow as tf\n",
    "import galsim\n",
    "from galsim import Image\n",
    "import galsim.hsm\n",
    "import galflow as gf\n",
    "from galaxy2galaxy import problems\n",
    "\n",
    "# Functions\n",
    "\n",
    "def ir2tf_simplifie(imp_resp, shape):\n",
    "    \n",
    "\n",
    "    dim = 2\n",
    "    # Zero padding and fill\n",
    "    irpadded = np.zeros(shape)\n",
    "    irpadded[tuple([slice(0, s) for s in imp_resp.shape])] = imp_resp\n",
    "    # Roll for zero convention of the fft to avoid the phase\n",
    "    # problem. Work with odd and even size.\n",
    "    for axis, axis_size in enumerate(imp_resp.shape):\n",
    "\n",
    "        irpadded = np.roll(irpadded,\n",
    "                           shift=-int(np.floor(axis_size / 2)),\n",
    "                           axis=axis)\n",
    "\n",
    "    return fft.rfftn(irpadded, axes=range(-dim, 0))\n",
    "\n",
    "def laplacian_simplifie(shape):\n",
    "    \n",
    "    impr = np.zeros([3,3])\n",
    "    for dim in range(2):\n",
    "        idx = tuple([slice(1, 2)] * dim +\n",
    "                    [slice(None)] +\n",
    "                    [slice(1, 2)] * (1 - dim))\n",
    "        impr[idx] = np.array([-1.0,\n",
    "                              0.0,\n",
    "                              -1.0]).reshape([-1 if i == dim else 1\n",
    "                                              for i in range(2)])\n",
    "    impr[(slice(1, 2), ) * 2] = 4.0\n",
    "    return ir2tf_simplifie(impr, shape), impr\n",
    "\n",
    "def laplacian_tf(shape):\n",
    "    return tf.convert_to_tensor(laplacian_simplifie(shape)[0])\n",
    "\n",
    "def wiener_tf(image, psf, balance, laplacian=True):\n",
    "    r\"\"\"Applies Wiener filter to image.\n",
    "\n",
    "    This function takes an image in the direct space and its corresponding PSF in the\n",
    "    Fourier space and performs a deconvolution using the Wiener Filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image   : 2D TensorFlow tensor\n",
    "        Image in the direct space.\n",
    "    psf     : 2D TensorFlow tensor\n",
    "        PSF in the Fourier space (or K space).\n",
    "    balance : scalar\n",
    "        Weight applied to regularization.\n",
    "    laplacian : boolean\n",
    "        If true the Laplacian regularization is used else the identity regularization \n",
    "        is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The first element is the filtered image in the Fourier space.\n",
    "        The second element is the PSF in the Fourier space (also know as the Transfer\n",
    "        Function).\n",
    "    \"\"\"\n",
    "    trans_func = psf\n",
    "    if laplacian:\n",
    "        reg = laplacian_tf(image.shape)\n",
    "        if psf.shape != reg.shape:\n",
    "            trans_func = tf.signal.rfft2d(tf.signal.ifftshift(tf.cast(psf, 'float32')))\n",
    "        else:\n",
    "            trans_func = psf\n",
    "    \n",
    "    arg1 = tf.cast(tf.math.conj(trans_func), 'complex64')\n",
    "    arg2 = tf.dtypes.cast(tf.math.abs(trans_func),'complex64') ** 2\n",
    "    arg3 = balance\n",
    "    if laplacian:\n",
    "        arg3 *= tf.dtypes.cast(tf.math.abs(laplacian_tf(image.shape)), 'complex64')**2\n",
    "    wiener_filter = arg1 / (arg2 + arg3)\n",
    "    \n",
    "    # Apply wiener in Foutier (or K) space\n",
    "    wiener_applied = wiener_filter * tf.signal.rfft2d(tf.cast(image, 'float32'))\n",
    "    \n",
    "    return wiener_applied, trans_func\n",
    "\n",
    "def pre_proc_unet(dico):\n",
    "    r\"\"\"Preprocess the data and apply the Tikhonov filter on the input galaxy images.\n",
    "\n",
    "    This function takes the dictionnary of galaxy images and PSF for the input and\n",
    "    the target and returns a list containing 2 arrays: an array of galaxy images that\n",
    "    are the output of the Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dico : dictionnary\n",
    "        Array_like means all those objects -- lists, nested lists, etc. --\n",
    "        that can be converted to an array.  We can also refer to\n",
    "        variables like `var1`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list containing 2 arrays: an array of galaxy images that are the output of the\n",
    "        Tikhonov filter and an array of target galaxy images.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    These are written in doctest format, and should illustrate how to\n",
    "    use the function.\n",
    "\n",
    "    >>> from galaxy2galaxy import problems # to list avaible problems run problems.available()\n",
    "    >>> problem128 = problems.problem('attrs2img_cosmos_hst2euclide')\n",
    "    >>> dset = problem128.dataset(Modes.TRAIN, data_dir='attrs2img_cosmos_hst2euclide')\n",
    "    >>> dset = dset.map(pre_proc_unet)\n",
    "    \"\"\"\n",
    "    # First, we add noise\n",
    "    # For the estimation of CFHT noise standard deviation check section 3 of:\n",
    "    # https://github.com/CosmoStat/ShapeDeconv/blob/master/data/CFHT/HST2CFHT.ipynb\n",
    "    sigma_cfht = 23.59\n",
    "    noise = tf.random_normal(shape=tf.shape(dico['inputs']), mean=0.0, stddev=sigma_cfht, dtype=tf.float32)\n",
    "    dico['inputs'] = dico['inputs'] + noise\n",
    "\n",
    "    # Second, we interpolate the image on a finer grid\n",
    "    x_interpolant=tf.image.ResizeMethod.BICUBIC\n",
    "    interp_factor = 2\n",
    "    Nx = 64\n",
    "    Ny = 64\n",
    "    dico['inputs_cfht'] = tf.image.resize(dico['inputs'],\n",
    "                    [Nx*interp_factor,\n",
    "                    Ny*interp_factor],\n",
    "                    method=x_interpolant)\n",
    "    # Since we lower the resolution of the image, we also scale the flux\n",
    "    # accordingly\n",
    "    dico['inputs_cfht'] = dico['inputs_cfht'] / interp_factor**2\n",
    "\n",
    "    balance = 9e-3  # determined using line search\n",
    "    dico['inputs_tikho'], _ = wiener_tf(dico['inputs_cfht'][...,0], dico['psf_cfht'][...,0], balance)\n",
    "    dico['inputs_tikho'] = tf.expand_dims(dico['inputs_tikho'], axis=0)\n",
    "    psf_hst = tf.reshape(dico['psf_hst'], [dico['psf_hst'].shape[-1],*dico['psf_hst'].shape[:2]])\n",
    "    psf_hst = tf.cast(psf_hst, 'complex64')\n",
    "    # gf.kconvolve performs a convolution in the K (Fourier) space\n",
    "    # inputs are given in K space\n",
    "    # the output is in the direct space\n",
    "    dico['inputs_tikho'] = gf.kconvolve(dico['inputs_tikho'], psf_hst,zero_padding_factor=1,interp_factor=interp_factor)\n",
    "    dico['inputs_tikho'] = dico['inputs_tikho'][0,...]\n",
    "\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /linkhome/rech/gencea01/uze68md/.local/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py:651: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading data files from /gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/attrs2img_cosmos_cfht2hst-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 3\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /gpfslocalsup/pub/anaconda-py3/2019.10/envs/tensorflow-gpu-1.15.2/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create an instance of the hsc_problem\n",
    "Modes = tf.estimator.ModeKeys\n",
    "problem128 = problems.problem('attrs2img_cosmos_cfht2hst')\n",
    "dset = problem128.dataset(Modes.EVAL, data_dir='/gpfswork/rech/xdy/uze68md/data/attrs2img_cosmos_cfht2hst/')\n",
    "dset = dset.repeat()\n",
    "dset = dset.map(pre_proc_unet)\n",
    "\n",
    "n_batch = 128\n",
    "\n",
    "dset = dset.batch(n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an iterator over the dataset\n",
    "iterator = dset.make_one_shot_iterator().get_next()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize batch\n",
    "batch = sess.run(iterator)\n",
    "k_batch = 32 #16\n",
    "\n",
    "for k in range(k_batch-1):\n",
    "    # Extract temporary batch\n",
    "    tmp = sess.run(iterator)\n",
    "    # Add to batch\n",
    "    for key in tmp:\n",
    "        batch[key]=np.concatenate((batch[key],tmp[key]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
